library(tidymodels)
library(vroom)
library(embed)
library(discrim)
library(themis)

train <- vroom("train.csv")
test <- vroom("test.csv")

my_recipe <- recipe(type ~ ., data=train) %>%
  step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
  step_smote(type) %>%
  step_normalize(all_numeric_predictors())
prep <- prep(my_recipe)
baked <- bake(prep, new_data = NULL)

# my_mod <- rand_forest(mtry = tune(),
#                       min_n = tune(),
#                       trees = 500) %>%
#   set_engine("ranger") %>%
#   set_mode("classification")

nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
   set_mode("classification") %>%
   set_engine("naivebayes") # install discrim library for the naiveb

ghost_workflow <- workflow() %>%
  add_recipe(my_recipe) %>%
  add_model(nb_model)

# tuning_grid <- grid_regular(mtry(range = c(1, ncol(train)-1)), min_n(), levels = 5)

tuning_grid <- grid_regular(Laplace(range = c(0, 2)), smoothness(range = c(.1, 1)), levels = 5)

folds <- vfold_cv(train, v = 5, repeats=1)

CV_results <- ghost_workflow %>%
  tune_grid(resamples=folds,
  grid=tuning_grid,
  metrics=metric_set(roc_auc))

bestTune <- CV_results %>%
  select_best()

final_wf <- ghost_workflow %>%
  finalize_workflow(bestTune) %>%
  fit(data=train)

preds <- predict(final_wf, new_data = test, type = "class")


kaggle_submission <- preds %>%
  bind_cols(test %>% select(id)) %>%
  select(id, .pred_class) %>%
  rename(type = .pred_class)

vroom_write(x=kaggle_submission, file="./ghostsubmission.csv", delim=",")
